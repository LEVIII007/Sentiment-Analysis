{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"notify_time":"5","kaggle":{"accelerator":"none","dataSources":[{"sourceId":7387881,"sourceType":"datasetVersion","datasetId":4294329}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/levii2375/us-airline-sentiment-analysis-project?scriptVersionId=164080676\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\n# from nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer","metadata":{"ExecuteTime":{"end_time":"2019-08-06T17:26:37.014475Z","start_time":"2019-08-06T17:26:22.708586Z"},"execution":{"iopub.status.busy":"2024-01-12T16:36:24.772459Z","iopub.execute_input":"2024-01-12T16:36:24.773094Z","iopub.status.idle":"2024-01-12T16:36:24.778765Z","shell.execute_reply.started":"2024-01-12T16:36:24.773066Z","shell.execute_reply":"2024-01-12T16:36:24.777363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:24.782463Z","iopub.execute_input":"2024-01-12T16:36:24.783236Z","iopub.status.idle":"2024-01-12T16:36:24.804824Z","shell.execute_reply.started":"2024-01-12T16:36:24.7832Z","shell.execute_reply":"2024-01-12T16:36:24.803861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/us-airline-sentiments/training_twitter_x_y_train.csv\") #10980 rows 12 cols\ntest = pd.read_csv(\"/kaggle/input/us-airline-sentiments/test_twitter_x_test.csv\")","metadata":{"ExecuteTime":{"end_time":"2019-08-06T17:26:38.585669Z","start_time":"2019-08-06T17:26:38.449887Z"},"execution":{"iopub.status.busy":"2024-01-12T16:36:24.806572Z","iopub.execute_input":"2024-01-12T16:36:24.806881Z","iopub.status.idle":"2024-01-12T16:36:24.874435Z","shell.execute_reply.started":"2024-01-12T16:36:24.806855Z","shell.execute_reply":"2024-01-12T16:36:24.873438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:24.875522Z","iopub.execute_input":"2024-01-12T16:36:24.875794Z","iopub.status.idle":"2024-01-12T16:36:24.889218Z","shell.execute_reply.started":"2024-01-12T16:36:24.875771Z","shell.execute_reply":"2024-01-12T16:36:24.887906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:24.891168Z","iopub.execute_input":"2024-01-12T16:36:24.891552Z","iopub.status.idle":"2024-01-12T16:36:24.910119Z","shell.execute_reply.started":"2024-01-12T16:36:24.891519Z","shell.execute_reply":"2024-01-12T16:36:24.908939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train['negativereason_gold'].nunique())\nprint(train['negativereason_gold'].value_counts(),\"\\n\")\n\nprint(test.negativereason_gold.nunique())\nprint(test.negativereason_gold.value_counts())","metadata":{"ExecuteTime":{"end_time":"2019-08-04T12:24:24.064191Z","start_time":"2019-08-04T12:24:24.059231Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2024-01-12T16:36:24.912847Z","iopub.execute_input":"2024-01-12T16:36:24.913206Z","iopub.status.idle":"2024-01-12T16:36:24.92225Z","shell.execute_reply.started":"2024-01-12T16:36:24.913173Z","shell.execute_reply":"2024-01-12T16:36:24.921321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_value_counts = train['airline_sentiment'].value_counts()\nprint(unique_value_counts)","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:24.923369Z","iopub.execute_input":"2024-01-12T16:36:24.923613Z","iopub.status.idle":"2024-01-12T16:36:24.936537Z","shell.execute_reply.started":"2024-01-12T16:36:24.923591Z","shell.execute_reply":"2024-01-12T16:36:24.935852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import imblearn","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:24.937393Z","iopub.execute_input":"2024-01-12T16:36:24.937701Z","iopub.status.idle":"2024-01-12T16:36:24.95212Z","shell.execute_reply.started":"2024-01-12T16:36:24.937673Z","shell.execute_reply":"2024-01-12T16:36:24.950931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from imblearn.over_sampling import RandomOverSampler\n\n# # Assuming 'df' is your DataFrame and 'airline_sentiment' is the target column\n# # Replace 'positive' and 'neutral' with your actual class names\n\n# # Separate the dataset into features and target\n# X = train.drop('airline_sentiment', axis=1)  # Features\n# y = train['airline_sentiment']  # Target\n\n# # Initialize RandomOverSampler\n# oversampler = RandomOverSampler(sampling_strategy={'positive': 5000, 'neutral': 5000}, random_state=42)\n\n# # Resample the dataset\n# X_resampled, y_resampled = oversampler.fit_resample(X, y)\n\n# # Combine the resampled data into a new DataFrame\n# resampled_df = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled, name='airline_sentiment')], axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:24.953635Z","iopub.execute_input":"2024-01-12T16:36:24.954175Z","iopub.status.idle":"2024-01-12T16:36:24.966616Z","shell.execute_reply.started":"2024-01-12T16:36:24.954135Z","shell.execute_reply":"2024-01-12T16:36:24.965562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# unique_value_counts = y_resampled.value_counts()\n# print(unique_value_counts)","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:24.96836Z","iopub.execute_input":"2024-01-12T16:36:24.969407Z","iopub.status.idle":"2024-01-12T16:36:24.981892Z","shell.execute_reply.started":"2024-01-12T16:36:24.969347Z","shell.execute_reply":"2024-01-12T16:36:24.980293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cleaning","metadata":{}},{"cell_type":"code","source":"drop_cols = ['airline_sentiment_gold','name','tweet_id', 'retweet_count','tweet_created','user_timezone','tweet_coord','tweet_location']\ntrain.drop(drop_cols, axis = 1, inplace=True)\ntest.drop(drop_cols, axis = 1, inplace=True)","metadata":{"ExecuteTime":{"end_time":"2019-08-06T17:26:40.96189Z","start_time":"2019-08-06T17:26:40.947305Z"},"execution":{"iopub.status.busy":"2024-01-12T16:36:24.984543Z","iopub.execute_input":"2024-01-12T16:36:24.984913Z","iopub.status.idle":"2024-01-12T16:36:24.996776Z","shell.execute_reply.started":"2024-01-12T16:36:24.984885Z","shell.execute_reply":"2024-01-12T16:36:24.995801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# stops = stopwords.words('english')\nstops = [\n    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \n    \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", \n    'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', \n    'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', \n    'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', \n    'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', \n    'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', \n    'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', \n    'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', \n    'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', \n    't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \n    've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \n    \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \n    \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \n    \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"\n]\n\nprint(stops)","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:24.998047Z","iopub.execute_input":"2024-01-12T16:36:24.998511Z","iopub.status.idle":"2024-01-12T16:36:25.012396Z","shell.execute_reply.started":"2024-01-12T16:36:24.998482Z","shell.execute_reply":"2024-01-12T16:36:25.011001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# stops = stopwords.words('english')\nstops += list(punctuation)\nstops += ['flight','airline','flights','AA']","metadata":{"ExecuteTime":{"end_time":"2019-08-06T17:26:41.612911Z","start_time":"2019-08-06T17:26:41.597341Z"},"execution":{"iopub.status.busy":"2024-01-12T16:36:25.013716Z","iopub.execute_input":"2024-01-12T16:36:25.013954Z","iopub.status.idle":"2024-01-12T16:36:25.028715Z","shell.execute_reply.started":"2024-01-12T16:36:25.013932Z","shell.execute_reply":"2024-01-12T16:36:25.027377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"abbreviations = {'ppl': 'people','cust':'customer','serv':'service','mins':'minutes','hrs':'hours','svc': 'service',\n           'u':'you','pls':'please'}\n\ntrain_index = train[~train.negativereason_gold.isna()].index\ntest_index = test[~test.negativereason_gold.isna()].index\n\nfor index, row in train.iterrows():\n    tweet = row.text\n    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet) #remove links\n    tweet = re.sub('@[^\\s]+','',tweet) #remove usernames\n    tweet = re.sub('[\\s]+', ' ', tweet) #remove additional whitespaces\n    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) #replace #word with word\n    tweet = tweet.strip('\\'\"') #trim tweet\n    words = []\n    for word in tweet.split():    \n#         if not hasNumbers(word):\n        if word.lower() not in stops:\n            if word in list(abbreviations.keys()):\n                words.append(abbreviations[word])\n            else:\n                words.append(word.lower())   \n    tweet = \" \".join(words)\n    tweet = \" %s %s\" % (tweet, row.airline)\n    row.text = tweet\n    if index in train_index:\n        row.text = \" %s %s\" % (row.text, row.negativereason_gold)\n\nfor index, row in test.iterrows():\n    tweet = row.text\n    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet) #remove links\n    tweet = re.sub('@[^\\s]+','',tweet) #remove usernames\n    tweet = re.sub('[\\s]+', ' ', tweet) #remove additional whitespaces\n    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) #replace #word with word\n    tweet = tweet.strip('\\'\"') #trim tweet\n    words = []\n    for word in tweet.split(): \n#         if not hasNumbers(word):\n        if word.lower() not in stops:\n            if word in list(abbreviations.keys()):\n                words.append(abbreviations[word])\n            else:\n                words.append(word.lower())\n    tweet = \" \".join(words)\n    tweet = \" %s %s\" % (tweet, row.airline)\n    row.text = tweet\n    if index in test_index:\n        row.text = \" %s %s\" % (row.text, row.negativereason_gold)\n\ndel train['negativereason_gold']\ndel test['negativereason_gold']","metadata":{"ExecuteTime":{"end_time":"2019-08-06T17:26:47.245154Z","start_time":"2019-08-06T17:26:42.658559Z"},"execution":{"iopub.status.busy":"2024-01-12T16:36:25.030367Z","iopub.execute_input":"2024-01-12T16:36:25.030955Z","iopub.status.idle":"2024-01-12T16:36:27.515154Z","shell.execute_reply.started":"2024-01-12T16:36:25.03092Z","shell.execute_reply":"2024-01-12T16:36:27.514221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def deEmojify(inputString):\n    return inputString.encode('ascii', 'ignore').decode('ascii')\n\nfor index, row in train.iterrows():\n    row.text = deEmojify(row.text)\n\nfor index, row in test.iterrows():\n    row.text = deEmojify(row.text)","metadata":{"ExecuteTime":{"end_time":"2019-08-06T17:26:52.848188Z","start_time":"2019-08-06T17:26:50.534082Z"},"code_folding":[],"execution":{"iopub.status.busy":"2024-01-12T16:36:27.520403Z","iopub.execute_input":"2024-01-12T16:36:27.52073Z","iopub.status.idle":"2024-01-12T16:36:28.742107Z","shell.execute_reply.started":"2024-01-12T16:36:27.520704Z","shell.execute_reply":"2024-01-12T16:36:28.740361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def hasNumbers(inputString):\n    return any(char.isdigit() for char in inputString)\n\nfor index, row in train.iterrows():\n    words = row.text.split()\n    new_words = []\n    for word in words:\n        if not hasNumbers(word):\n            new_words.append(word)\n    row.text = \" \".join(new_words)\n    \nfor index, row in test.iterrows():\n    words = row.text.split()\n    new_words = []\n    for word in words:\n        if not hasNumbers(word):\n            new_words.append(word)\n    row.text = \" \".join(new_words)","metadata":{"ExecuteTime":{"end_time":"2019-08-06T17:26:55.482112Z","start_time":"2019-08-06T17:26:52.850905Z"},"execution":{"iopub.status.busy":"2024-01-12T16:36:28.743619Z","iopub.execute_input":"2024-01-12T16:36:28.743985Z","iopub.status.idle":"2024-01-12T16:36:30.212549Z","shell.execute_reply.started":"2024-01-12T16:36:28.743959Z","shell.execute_reply":"2024-01-12T16:36:30.211321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Oversampling positive and neutral tweets**","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler\n\n# Assuming 'df' is your DataFrame and 'airline_sentiment' is the target column\n# Replace 'positive' and 'neutral' with your actual class names\n\n# Separate the dataset into features and target\nX = train.drop('airline_sentiment', axis=1)  # Features\ny = train['airline_sentiment']  # Target\n\n# Initialize RandomOverSampler\noversampler = RandomOverSampler(sampling_strategy={'positive': 5000, 'neutral': 5000}, random_state=42)\n\n# Resample the dataset\nX_resampled, y_resampled = oversampler.fit_resample(X, y)\n\n# Combine the resampled data into a new DataFrame\ntrain = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled, name='airline_sentiment')], axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:30.213765Z","iopub.execute_input":"2024-01-12T16:36:30.214068Z","iopub.status.idle":"2024-01-12T16:36:30.249612Z","shell.execute_reply.started":"2024-01-12T16:36:30.214041Z","shell.execute_reply":"2024-01-12T16:36:30.248491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"ExecuteTime":{"end_time":"2019-08-06T17:26:55.560184Z","start_time":"2019-08-06T17:26:55.485923Z"},"execution":{"iopub.status.busy":"2024-01-12T16:36:30.251183Z","iopub.execute_input":"2024-01-12T16:36:30.251859Z","iopub.status.idle":"2024-01-12T16:36:30.261352Z","shell.execute_reply.started":"2024-01-12T16:36:30.251834Z","shell.execute_reply":"2024-01-12T16:36:30.259928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nunique_value_counts = train['airline_sentiment'].value_counts()\nprint(unique_value_counts)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:30.262679Z","iopub.execute_input":"2024-01-12T16:36:30.263049Z","iopub.status.idle":"2024-01-12T16:36:30.276609Z","shell.execute_reply.started":"2024-01-12T16:36:30.263021Z","shell.execute_reply":"2024-01-12T16:36:30.275886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating vocab and data formatting","metadata":{}},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:30.277548Z","iopub.execute_input":"2024-01-12T16:36:30.277857Z","iopub.status.idle":"2024-01-12T16:36:30.293957Z","shell.execute_reply.started":"2024-01-12T16:36:30.277828Z","shell.execute_reply":"2024-01-12T16:36:30.292446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['text'][3]","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:30.295164Z","iopub.execute_input":"2024-01-12T16:36:30.29548Z","iopub.status.idle":"2024-01-12T16:36:30.307699Z","shell.execute_reply.started":"2024-01-12T16:36:30.295455Z","shell.execute_reply":"2024-01-12T16:36:30.306534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**model training using logistive regression**","metadata":{}},{"cell_type":"markdown","source":"***TFID vectorization to convert text to numbers***","metadata":{}},{"cell_type":"code","source":"v = TfidfVectorizer(analyzer='word', max_features=3150, max_df = 0.8, ngram_range=(1,1))\ntrain_features= v.fit_transform(train.text)\ntest_features=v.transform(test.text)","metadata":{"ExecuteTime":{"end_time":"2019-08-06T17:56:34.904385Z","start_time":"2019-08-06T17:56:34.631243Z"},"execution":{"iopub.status.busy":"2024-01-12T16:36:30.309255Z","iopub.execute_input":"2024-01-12T16:36:30.309523Z","iopub.status.idle":"2024-01-12T16:36:30.536949Z","shell.execute_reply.started":"2024-01-12T16:36:30.3095Z","shell.execute_reply":"2024-01-12T16:36:30.535942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Assuming 'df' is your DataFrame and 'processed_text' is the column with processed text\nX =  train_features\ny = train['airline_sentiment']   # Replace 'target_column' with the actual target column name\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:30.538381Z","iopub.execute_input":"2024-01-12T16:36:30.538631Z","iopub.status.idle":"2024-01-12T16:36:30.548416Z","shell.execute_reply.started":"2024-01-12T16:36:30.538608Z","shell.execute_reply":"2024-01-12T16:36:30.54683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n# Instantiate Logistic Regression model\nclf = LogisticRegression(C=2.1, solver='liblinear', multi_class='auto')\n\n# Fit the model on the training data\nclf.fit(X_train,y_train)\n\n# Make predictions on the test data\npred = clf.predict(X_test)\npred1 = clf.predict(X_train)\n\n# Save predictions to a CSV file\npredictions_df = pd.DataFrame({'Prediction': pred})\npredictions_df.to_csv('predictions_twitter.csv', index=False)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, pred)\nclassification_rep = classification_report(y_test, pred)\nconfusion_mat = confusion_matrix(y_test, pred)\n\n# Print evaluation metrics\nprint(f'Accuracy: {accuracy:.4f}')\nprint('\\nClassification Report:\\n', classification_rep)\nprint('\\nConfusion Matrix:\\n', confusion_mat)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:30.550366Z","iopub.execute_input":"2024-01-12T16:36:30.550842Z","iopub.status.idle":"2024-01-12T16:36:30.815056Z","shell.execute_reply.started":"2024-01-12T16:36:30.550762Z","shell.execute_reply":"2024-01-12T16:36:30.814192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = accuracy_score(y_train, pred1)\nclassification_rep = classification_report(y_train, pred1)\nconfusion_mat = confusion_matrix(y_train, pred1)\n\n# Print evaluation metrics\nprint(f'Accuracy: {accuracy:.4f}')\nprint('\\nClassification Report:\\n', classification_rep)\nprint('\\nConfusion Matrix:\\n', confusion_mat)","metadata":{"ExecuteTime":{"end_time":"2019-08-06T17:56:38.481957Z","start_time":"2019-08-06T17:56:38.333543Z"},"execution":{"iopub.status.busy":"2024-01-12T16:39:14.063756Z","iopub.execute_input":"2024-01-12T16:39:14.064137Z","iopub.status.idle":"2024-01-12T16:39:14.421185Z","shell.execute_reply.started":"2024-01-12T16:39:14.06411Z","shell.execute_reply":"2024-01-12T16:39:14.419854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Predictions on the test set\n\n# Compute the confusion matrix\ncm_lr = confusion_matrix(y_test, pred)\n\n# Plot the confusion matrix as a heatmap\nsns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix(logistic regression)')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:51:43.587947Z","iopub.execute_input":"2024-01-12T16:51:43.588318Z","iopub.status.idle":"2024-01-12T16:51:43.827556Z","shell.execute_reply.started":"2024-01-12T16:51:43.588282Z","shell.execute_reply":"2024-01-12T16:51:43.826568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***model training using SVM, using rbf kernel***","metadata":{}},{"cell_type":"code","source":"clf = SVC(kernel=\"rbf\", C= 0.9 , gamma = 'scale')\n# clf = SVC(C = 1000, gamma = 0.001)\nclf.fit(X_train, y_train)\npred = clf.predict(X_test)\npred1 = clf.predict(X_train)","metadata":{"ExecuteTime":{"end_time":"2019-08-06T15:53:29.055261Z","start_time":"2019-08-06T15:53:19.146191Z"},"execution":{"iopub.status.busy":"2024-01-12T16:51:51.273201Z","iopub.execute_input":"2024-01-12T16:51:51.273687Z","iopub.status.idle":"2024-01-12T16:52:15.372534Z","shell.execute_reply.started":"2024-01-12T16:51:51.273629Z","shell.execute_reply":"2024-01-12T16:52:15.371463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model\naccuracy = accuracy_score(y_test, pred)\nclassification_rep = classification_report(y_test, pred)\nconfusion_mat = confusion_matrix(y_test, pred)\n\n# Print evaluation metrics\nprint(f'Accuracy: {accuracy:.4f}')\nprint('\\nClassification Report:\\n', classification_rep)\nprint('\\nConfusion Matrix:\\n', confusion_mat)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:55.495796Z","iopub.execute_input":"2024-01-12T16:36:55.49613Z","iopub.status.idle":"2024-01-12T16:36:55.589627Z","shell.execute_reply.started":"2024-01-12T16:36:55.4961Z","shell.execute_reply":"2024-01-12T16:36:55.588621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = accuracy_score(y_train, pred1)\nclassification_rep = classification_report(y_train, pred1)\nconfusion_mat = confusion_matrix(y_train, pred1)\n# Print evaluation metrics\nprint(f'Accuracy: {accuracy:.4f}')\nprint('\\nClassification Report:\\n', classification_rep)\nprint('\\nConfusion Matrix:\\n', confusion_mat)","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:55.591262Z","iopub.execute_input":"2024-01-12T16:36:55.592172Z","iopub.status.idle":"2024-01-12T16:36:55.941231Z","shell.execute_reply.started":"2024-01-12T16:36:55.592138Z","shell.execute_reply":"2024-01-12T16:36:55.940302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('predictions_twitter2.csv', 'w') as f: #less accurate\n    for item in pred:\n        f.write(\"%s\\n\" % item)","metadata":{"ExecuteTime":{"end_time":"2019-08-06T15:53:29.649946Z","start_time":"2019-08-06T15:53:29.638435Z"},"execution":{"iopub.status.busy":"2024-01-12T16:36:55.942505Z","iopub.execute_input":"2024-01-12T16:36:55.944586Z","iopub.status.idle":"2024-01-12T16:36:55.952027Z","shell.execute_reply.started":"2024-01-12T16:36:55.944558Z","shell.execute_reply":"2024-01-12T16:36:55.950437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Predictions on the test set\n\n# Compute the confusion matrix\ncm_svm = confusion_matrix(y_test, pred)\n# Plot the confusion matrix as a heatmap\nsns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix(SVM)')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:52:47.177307Z","iopub.execute_input":"2024-01-12T16:52:47.177686Z","iopub.status.idle":"2024-01-12T16:52:47.429056Z","shell.execute_reply.started":"2024-01-12T16:52:47.177633Z","shell.execute_reply":"2024-01-12T16:52:47.427322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Hypertuning SVM model**","metadata":{}},{"cell_type":"code","source":"# from sklearn.model_selection import GridSearchCV\n# from sklearn.svm import SVC\n\n# # Split your data into training and testing sets\n# # Define the SVM model\n\n# svm_model = SVC()\n\n# # Define the hyperparameters and their possible values for the grid search\n# param_grid = {\n#     'C': [0.1, 1, 10, 100],         # Regularization parameter\n#     'kernel': ['linear', 'rbf', 'poly'],  # Kernel type\n#     'gamma': [0.1, 1, 10],            # Kernel coefficient for 'rbf' and 'poly' kernels\n#     'degree': [2, 3, 4],              # Degree of the polynomial kernel ('poly' only)\n# }\n\n# # Perform grid search with cross-validation\n# grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=5, scoring='accuracy')\n# grid_search.fit(X_train, y_train)\n\n# # Display the best hyperparameters\n# best_params = grid_search.best_params_\n# print(\"Best Hyperparameters:\", best_params)\n\n# # Get the best SVM model\n# best_svm_model = grid_search.best_estimator_\n\n# # Evaluate the model on the test set\n# y_pred = best_svm_model.predict(X_test)\n\n# # Display classification report or other evaluation metrics\n# print(classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:56.002815Z","iopub.status.idle":"2024-01-12T16:36:56.003142Z","shell.execute_reply.started":"2024-01-12T16:36:56.002987Z","shell.execute_reply":"2024-01-12T16:36:56.003002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# v.get_feature_names()","metadata":{"ExecuteTime":{"end_time":"2019-08-04T10:09:48.615243Z","start_time":"2019-08-04T10:09:48.589411Z"},"code_folding":[],"execution":{"iopub.status.busy":"2024-01-12T16:36:56.005635Z","iopub.status.idle":"2024-01-12T16:36:56.007035Z","shell.execute_reply.started":"2024-01-12T16:36:56.006761Z","shell.execute_reply":"2024-01-12T16:36:56.006791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:56.008332Z","iopub.status.idle":"2024-01-12T16:36:56.00949Z","shell.execute_reply.started":"2024-01-12T16:36:56.009248Z","shell.execute_reply":"2024-01-12T16:36:56.009277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**simple neural network model**","metadata":{}},{"cell_type":"code","source":"import tensorflow","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:56.010405Z","iopub.status.idle":"2024-01-12T16:36:56.01206Z","shell.execute_reply.started":"2024-01-12T16:36:56.01168Z","shell.execute_reply":"2024-01-12T16:36:56.011728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"y = train['airline_sentiment']  # Map labels to numerical values","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:56.013094Z","iopub.status.idle":"2024-01-12T16:36:56.013816Z","shell.execute_reply.started":"2024-01-12T16:36:56.0136Z","shell.execute_reply":"2024-01-12T16:36:56.013617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\n\nX =  train['text']\ny = train['airline_sentiment'].map({'negative': 0, 'neutral': 1, 'positive': 2})   \n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Tokenize the text\nmax_words = 10000  \ntokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(X_train)\n\n\nX_train_sequences = tokenizer.texts_to_sequences(X_train)\nX_test_sequences = tokenizer.texts_to_sequences(X_test)\n\n# Pad sequences to have the same length\nmax_sequence_length = 100  # Adjust the maximum sequence length based on your data\nX_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\nX_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n\n# Print the processed data\nprint(\"X_train_padded shape:\", X_train_padded.shape)\nprint(\"X_test_padded shape:\", X_test_padded.shape)\nprint(\"X_train_padded example:\", X_train_padded[0])\n\n# Now, X_train_padded and X_test_padded are ready to be used for neural network training\n","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:56.015532Z","iopub.status.idle":"2024-01-12T16:36:56.01605Z","shell.execute_reply.started":"2024-01-12T16:36:56.015831Z","shell.execute_reply":"2024-01-12T16:36:56.015853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_padded","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:56.017232Z","iopub.status.idle":"2024-01-12T16:36:56.017717Z","shell.execute_reply.started":"2024-01-12T16:36:56.017483Z","shell.execute_reply":"2024-01-12T16:36:56.017503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout\n\n# Create a Sequential model\nmodel = Sequential()\n\n# Add an Embedding layer\nembedding_dim = 16\nmodel.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\n\n# Flatten the output of the Embedding layer\nmodel.add(Flatten())\nmodel.add(Dropout(0.2))\n\n# Add a Dense hidden layer with ReLU activation\nmodel.add(Dense(units=64, activation='relu'))\nmodel.add(Dropout(0.2))\n\n# Add another Dense hidden layer with ReLU activation\nmodel.add(Dense(units=32, activation='relu'))\nmodel.add(Dropout(0.2))\n\n# Add a Dense output layer with softmax activation for multi-class classification\nnum_classes = 3\nmodel.add(Dense(num_classes, activation='softmax'))\n\n# Compile the model using the Adam optimizer and sparse categorical crossentropy loss\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Display the model summary\nmodel.summary()\n\n# Train the model on the training data\nmodel.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_split=0.2)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:56.019397Z","iopub.status.idle":"2024-01-12T16:36:56.019909Z","shell.execute_reply.started":"2024-01-12T16:36:56.019631Z","shell.execute_reply":"2024-01-12T16:36:56.019674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss, accuracy = model.evaluate(X_train_padded, y_train)\nprint(f'Test Loss: {loss:.4f}')\nprint(f'Test Accuracy: {accuracy * 100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:56.021027Z","iopub.status.idle":"2024-01-12T16:36:56.021465Z","shell.execute_reply.started":"2024-01-12T16:36:56.021243Z","shell.execute_reply":"2024-01-12T16:36:56.021274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss, accuracy = model.evaluate(X_test_padded, y_test)\nprint(f'Test Loss: {loss:.4f}')\nprint(f'Test Accuracy: {accuracy * 100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:56.022891Z","iopub.status.idle":"2024-01-12T16:36:56.023322Z","shell.execute_reply.started":"2024-01-12T16:36:56.023103Z","shell.execute_reply":"2024-01-12T16:36:56.023123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**using LSTM layers in place of reLu activation layer**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n\n\nmodel2 = Sequential()\n\n# Add an Embedding layer\nembedding_dim = 16\nmodel2.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\n\n\nmodel2.add(LSTM(units=64, return_sequences=True))\nmodel2.add(Dropout(0.2))\n\n\nmodel2.add(LSTM(units=32))\nmodel2.add(Dropout(0.2))\n\n\nnum_classes = 3 \nmodel2.add(Dense(num_classes, activation='softmax'))\n\n\nmodel2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n\nmodel2.summary()\n\n\nmodel2.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_split=0.2)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:56.024606Z","iopub.status.idle":"2024-01-12T16:36:56.025075Z","shell.execute_reply.started":"2024-01-12T16:36:56.02487Z","shell.execute_reply":"2024-01-12T16:36:56.02489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2.summary()","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:56.02599Z","iopub.status.idle":"2024-01-12T16:36:56.02639Z","shell.execute_reply.started":"2024-01-12T16:36:56.026182Z","shell.execute_reply":"2024-01-12T16:36:56.0262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yprid = model2.predict(X_test_padded)","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:56.028068Z","iopub.status.idle":"2024-01-12T16:36:56.028475Z","shell.execute_reply.started":"2024-01-12T16:36:56.028279Z","shell.execute_reply":"2024-01-12T16:36:56.028298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss, accuracy = model2.evaluate(X_train_padded, y_train)\nprint(f'Test Loss: {loss:.4f}')\nprint(f'Test Accuracy: {accuracy * 100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:56.029625Z","iopub.status.idle":"2024-01-12T16:36:56.030035Z","shell.execute_reply.started":"2024-01-12T16:36:56.029841Z","shell.execute_reply":"2024-01-12T16:36:56.029861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss, accuracy = model.evaluate(X_test_padded, y_test)\nprint(f'Test Loss: {loss:.4f}')\nprint(f'Test Accuracy: {accuracy * 100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:56.031475Z","iopub.status.idle":"2024-01-12T16:36:56.031922Z","shell.execute_reply.started":"2024-01-12T16:36:56.031711Z","shell.execute_reply":"2024-01-12T16:36:56.031729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now let's try it now on a completely new data","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:56.033036Z","iopub.status.idle":"2024-01-12T16:36:56.033441Z","shell.execute_reply.started":"2024-01-12T16:36:56.033231Z","shell.execute_reply":"2024-01-12T16:36:56.03325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtest =  test['text']\n#ytest = test['airline'].map({'negative': 0, 'neutral': 1, 'positive': 2})   # Replace 'target_column' with the actual target column name\n\n# Split the data into training and testing sets\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nmax_words = 10000  # Adjust the maximum number of words to consider based on your data\ntokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(Xtest)\n\n# Convert text to sequences of integers\nX_test_sequences = tokenizer.texts_to_sequences(Xtest)\n\n# Pad sequences to have the same length\nmax_sequence_length = 100  # Adjust the maximum sequence length based on your data\nX_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n#X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:56.035132Z","iopub.status.idle":"2024-01-12T16:36:56.035538Z","shell.execute_reply.started":"2024-01-12T16:36:56.035335Z","shell.execute_reply":"2024-01-12T16:36:56.035353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yprid_test = model.predict(X_test_padded)","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:56.037092Z","iopub.status.idle":"2024-01-12T16:36:56.037496Z","shell.execute_reply.started":"2024-01-12T16:36:56.037295Z","shell.execute_reply":"2024-01-12T16:36:56.037314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yprid_test","metadata":{"execution":{"iopub.status.busy":"2024-01-12T16:36:56.039295Z","iopub.status.idle":"2024-01-12T16:36:56.039704Z","shell.execute_reply.started":"2024-01-12T16:36:56.039481Z","shell.execute_reply":"2024-01-12T16:36:56.039499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}